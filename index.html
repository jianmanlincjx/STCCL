<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Learning Spatial-Temporal Coherent Correlations for Speech-Preserving Facial Expression Manipulation</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 1000px;
      margin: auto;
      padding: 20px;
      line-height: 1.6;
    }
    h1, h2, h3 {
      text-align: center;
    }
    .authors {
      text-align: center;
      font-size: 16px;
      color: #555;
    }
    .buttons {
      text-align: center;
      margin: 20px 0;
    }
    .buttons a {
      text-decoration: none;
      margin: 0 10px;
      padding: 10px 20px;
      background: #007bff;
      color: white;
      border-radius: 5px;
    }
    .section {
      margin-top: 40px;
    }
  .video-row {
    display: grid;
    grid-template-columns: repeat(2, 1fr);
    gap: 20px;
    margin-bottom: 20px;
  }
  
  .video-container video {
    width: 100%;
    max-width: 720px;  /* æ”¾å¤§è§†é¢‘å®½åº¦ */
    height: auto;
  }
  .caption {
    font-size: 14px;
    color: #444;
    text-align: left;
    margin: 10px auto 20px auto;
    max-width: 900px;
    line-height: 1.5;
  }

  </style>
</head>
<body>

  <h1>Learning Spatial-Temporal Coherent Correlations for Speech-Preserving Facial Expression Manipulation</h1>

  <div class="authors">
    Tianshui Chen<sup>1</sup>, Jianman Lin<sup>2</sup>, Zhijiang Yang<sup>1</sup>, Chunmei Qing<sup>2</sup>, Liang Lin<sup>3</sup><br>
    <sup>1</sup>Guangdong University of Technology, <sup>2</sup>South China University of Technology, <sup>3</sup>Sun Yat-sen University
  </div>

<div class="buttons">
  <a href="#">ðŸ“„ Paper</a>
  <a href="https://github.com/jianmanlincjx/STCCL" target="_blank">ðŸ’» Code</a>
  <a href="https://drive.google.com/file/d/1jovMEXqp7ZCqh4NusY72JH0D_QjrdTxv/view?usp=drive_link" target="_blank">ðŸ“¦ Models</a>
</div>



  <div class="section">
    <h2>Abstract</h2>
    <p>
      Speech-preserving facial expression manipulation (SPFEM) aims to modify facial emotions while meticulously maintaining the mouth animation associated with spoken content. Current works depend on inaccessible paired training samples for the person, where two aligned frames exhibit the same speech content yet differ in emotional expression, limiting the SPFEM applications in real-world scenarios. In this work, we discover that speakers who convey the same content with different emotions exhibit highly correlated local facial animations in both spatial and temporal spaces, providing valuable supervision for SPFEM. To capitalize on this insight, we propose a novel spatial-tempral coherent correlation learning (STCCL) algorithm, which models the aforementioned correlations as explicit metrics and integrates the metrics to supervise manipulating facial expression and meanwhile better preserving the facial animation of spoken contents. To this end, it first learns a spatial coherent correlation metric, ensuring that the visual correlations of adjacent local regions within an image linked to a specific emotion closely resemble those of corresponding regions in an image linked to a different emotion. Simultaneously, it develops a temporal coherent correlation metric, ensuring that the visual correlations of specific regions across adjacent image frames associated with one emotion are similar to those in the corresponding regions of frames associated with another emotion. Recognizing that visual correlations are not uniform across all regions, we have also crafted a correlation-aware adaptive strategy that prioritizes regions that present greater challenges. During SPFEM model training, we construct the spatial-temporal coherent correlation metric between corresponding local regions of the input and output image frames as addition loss to supervise the generation process. We conduct extensive experiments on variant datasets, and the results demonstrate the effectiveness of the proposed STCCL algorithm.
    </p>
  </div>

  <div class="section">
    <h2>Integrate STCCL into NED</h2>
    <img src="STCCL_NED.png" alt="STCCL Framework" style="max-width: 95%; height: auto;">
    <div class="caption">
      An overall pipeline of incorporating the proposed STCCL algorithm to the current advanced NED method to supervise generating the intermediate 3DMM meshes and final rendered images. It computes visual correlation of corresponding and non-corresponding local regions between the source and generated images, followed by the correlation-aware adaptive strategy to obtain the final loss to supervise final image generation. An identical process is performed on the source and generated 3DMM meshes to supervise the intermediate 3DMM mesh generation.
    </div>
  </div>

  <div class="section">
    <h2>STCCL Algorithm Overview</h2>
    <img src="STCCL.png" alt="STCCL" style="max-width: 95%; height: auto;">
    <div class="caption">
      <b>Left half:</b> An illustration of spatial-temporal coherent correlation metric learning based on visual disparity. It retrieves the corresponding adjacent local regions of the input and output images (sequence) as positive samples and the non-corresponding counterparts as negative samples. The process is performed in the feature maps to construct dense positive and negative samples to train the metric.
      <b>Right half:</b> Similar to the <b>Left half</b>, but based on correlation matrices.
    </div>
  </div>

<div class="section">
  <h2>Video Examples</h2>

  <div class="video-row">
    <div class="video-container"><video controls src="video_result_base_vd/M012_source_happy_037.mp4"></video></div>
    <div class="video-container"><video controls src="video_result_base_vd/W015_source_happy_037.mp4"></video></div>
  </div>

  <div class="video-row">
    <div class="video-container"><video controls src="video_result_base_vd/W015_source_happy_039.mp4"></video></div>
    <div class="video-container"><video controls src="video_result_base_vd/M003_source_sad_033.mp4"></video></div>
  </div>

  <div class="video-row">
    <div class="video-container"><video controls src="video_result_base_vd/W015_source_sad_028.mp4"></video></div>
    <div class="video-container"><video controls src="video_result_base_vd/W015_source_sad_037.mp4"></video></div>
  </div>

  <div class="video-row">
    <div class="video-container"><video controls src="video_result_base_vd/M012_source_angry_029.mp4"></video></div>
    <div class="video-container"><video controls src="video_result_base_vd/M030_source_angry_037.mp4"></video></div>
  </div>

  <div class="video-row">
    <div class="video-container"><video controls src="video_result_base_vd/M012_source_neutral_040.mp4"></video></div>
    <div class="video-container"><video controls src="video_result_base_vd/M012_source_disgusted_028.mp4"></video></div>
  </div>

  <div class="video-row">
    <div class="video-container"><video controls src="video_result_base_vd/M030_source_disgusted_030.mp4"></video></div>
    <div class="video-container"><video controls src="video_result_base_vd/W015_source_sad_028.mp4"></video></div>
  </div>

  <p>For more video results, please visit: <a href="https://jianmanlincjx.github.io/STCCL_video_result/" target="_blank">https://jianmanlincjx.github.io/STCCL_video_result/</a></p>
</div>



</body>
</html>
