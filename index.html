<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>STCCL: Spatial-Temporal Cross Correlation Learning</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 900px;
      margin: auto;
      padding: 20px;
      line-height: 1.6;
    }
    h1, h2 {
      text-align: center;
    }
    .center {
      text-align: center;
    }
    .authors {
      text-align: center;
      font-size: 16px;
      color: #555;
    }
    .buttons {
      text-align: center;
      margin: 20px 0;
    }
    .buttons a {
      text-decoration: none;
      margin: 0 10px;
      padding: 10px 20px;
      background: #007bff;
      color: white;
      border-radius: 5px;
    }
    .video-row {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 20px;
      margin-top: 10px;
    }
    .video-container {
      text-align: center;
    }
    video {
      width: 220px;
      height: auto;
    }
    img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
    }
    .section {
      margin-top: 40px;
    }
  </style>
</head>
<body>

  <h1>STCCL: Spatial-Temporal Cross Correlation Learning</h1>
  <div class="authors">
    Your Name<sup>1</sup>, Coauthor A<sup>2</sup>, Coauthor B<sup>1</sup>, ...
    <br>
    <sup>1</sup>Institution One, <sup>2</sup>Institution Two
  </div>

  <div class="buttons">
    <a href="link_to_paper.pdf" target="_blank">ðŸ“„ Paper</a>
    <a href="https://github.com/your_repo" target="_blank">ðŸ’» Code</a>
    <a href="link_to_models" target="_blank">ðŸ“¦ Models</a>
    <a href="link_to_video_demo" target="_blank">ðŸŽ¥ Video</a>
  </div>

  <div class="section">
    <h2>Abstract</h2>
    <p>
      Speech-preserving facial expression manipulation (SPFEM) aims to automatically modify facial emotions while maintaining facial animations of the speech content. Current works depend on inaccessible paired data between expressions and utterances. We propose STCCL, a spatial-temporal cross-correlation learning module that supervises visual correlations for better alignment between facial expression generation and speech-driven regions.
    </p>
  </div>

  <div class="section">
    <h2>Framework</h2>
    <img src="framework.png" alt="Framework Diagram">
  </div>

  <div class="section">
    <h2>Video Examples</h2>

    <h3 class="center">Happy</h3>
    <div class="video-row">
      <div class="video-container">
        <video controls>
          <source src="happy1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="video-container">
        <video controls>
          <source src="happy2.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <h3 class="center">Sad</h3>
    <div class="video-row">
      <div class="video-container">
        <video controls>
          <source src="sad1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="video-container">
        <video controls>
          <source src="sad2.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>

</body>
</html>
